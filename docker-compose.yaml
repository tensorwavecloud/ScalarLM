services:

    cray: &cray
        command: /app/cray/scripts/start_one_server.sh
        build:
            context: .
            dockerfile: Dockerfile
            platforms:
                - ${DOCKER_PLATFORM:-linux/arm64}
            args:
                - TORCH_CUDA_ARCH_LIST=${TORCH_CUDA_ARCH_LIST}
                - BASE_NAME=${BASE_NAME}
                - VLLM_TARGET_DEVICE=${VLLM_TARGET_DEVICE}
                - VLLM_SOURCE=${VLLM_SOURCE:-remote}
                - VLLM_BRANCH=${VLLM_BRANCH:-main}
                - VLLM_REPO=${VLLM_REPO:-https://github.com/supermassive-intelligence/vllm-fork.git}
        ports:
            - "8000:8000"
            - "8001:8001"
        volumes:
            - type: bind
              source: ./models
              target: /root/.cache/huggingface
            - type: bind
              source: ./infra/cray_infra
              target: /app/cray/infra/cray_infra
            - type: bind
              source: ./scripts
              target: /app/cray/scripts
            - type: bind
              source: ./ml
              target: /app/cray/ml
            - type: bind
              source: ./test
              target: /app/cray/test
        networks:
            - cray-network


    cray-nvidia:
        <<: *cray
        runtime: nvidia
        restart: unless-stopped
        cap_add:
          - SYS_PTRACE
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          capabilities: [gpu]

    cray-amd:
        <<: *cray
        devices:
          - /dev/kfd
          - /dev/dri
        security_opt:
          - seccomp:unconfined
        cap_add:
          - SYS_PTRACE

networks:
  cray-network:
    name: cray_network

